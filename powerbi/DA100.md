Structure

 

 

Warehouse

## #1

You have a custom connector that returns ID, From, To, Subject, Body, and Has Attachments for every email sent during the past year. More than 10 million records are returned.

You build a report analyzing the internal networks of employees based on whom they send emails to.

You need to prevent report recipients from reading the analyzed emails. The solution must minimize the model size. What should you do?

 

A.      Implement row-level security (RLS) so that the report recipients can only see results based on the emails they sent.

B.      Remove the Subject and Body columns during the import.

C.      From Model view, set the Subject and Body columns to Hidden.

 

Correct Answer: B

Incorrect Answers:

A, C: Does not reduce the size of the model

## #2

You have the tables shown in the following table.

![Table  Description automatically generated](file:///C:/Users/V-WENB~1.FAR/AppData/Local/Temp/msohtmlclip1/01/clip_image001.png)

 

The Impressions table contains approximately 30 million records per month. You need to create an ad analytics system to meet the following requirements:

✑ Present ad impression counts for the day, campaign, and Site_name. The analytics for the last year are required.

✑ Minimize the data model size.

Which two actions should you perform? Each correct answer presents part of the solution. NOTE: Each correct selection is worth one point.

 

A.      Group the Impressions query in Power Query by Ad_id, Site_name, and Impression_date. Aggregate by using the CountRows function.

B.      Create one-to-many relationships between the tables.

C.      Create a calculated measure that aggregates by using the COUNTROWS function.

D.      Create a calculated table that contains Ad_id, Site_name, and Impression_date.

 

 

Correct Answer: AB

https://docs.microsoft.com/en-us/power-bi/guidance/import-modeling-data-reduction#group-by-and-summarize

A)      will minimize data model size because currently field Impression_time takes much space and generates many rows.

B)      is needed because we need to present report by campaign_name. As other information are in table Impressions we need relationship to table Campaigns to get campaign name.

Option D) will not minimize the model so it is for sure incorrect.

Option C) will partially work, it will work for counting Ad_id, but will need relationship "both" to group accurately by Campaign name.

## #3

Your company has training videos that are published to Microsoft Stream. You need to surface the videos directly in a Microsoft Power BI dashboard. Which type of tile should you add?

 

A.      video

B.      custom streaming data

C.      text box

D.      web content

 

The only way to visualize a streaming dataset is to add a tile and use the streaming dataset as a custom streaming data source. Reference:

https://docs.microsoft.com/en-us/power-bi/connect-data/service-real-time-streaming

https://docs.microsoft.com/en-us/stream/portal-embed-video

https://docs.microsoft.com/en-us/power-bi/create-reports/service-dashboard-add-widget#add-web-content

 

 

\#4

You open a query in Power Query Editor.

You need to identify the percentage of empty values in each column as quickly as possible. Which Data Preview option should you select?

 

A.      Show whitespace

B.      Column profile

C.      Column distribution

D.      Column quality

Correct Answer: D

Column quality: In this section, we can easily see valid, Error and Empty percentage of data values associated with the Selected table. Note: In Power Query Editor, Under View tab in Data Preview Section we can see the following data profiling functionalities:

✑ Column quality

✑ Column distribution

✑ Column profile Reference:

https://community.powerbi.com/t5/Community-Blog/Data-Profiling-in-Power-BI-Power-BI-Update-April-2019/ba-p/674555

https://docs.microsoft.com/en-us/power-query/data-profiling-tools

 

 

## #5

You have a prospective customer list that contains 1,500 rows of data. The list contains the following fields:

✑ First name

✑ Last name

✑ Email address

✑ State/Region

✑ Phone number

You import the list into Power Query Editor.

You need to ensure that the list contains records for each State/Region to which you want to target a marketing campaign. Which two actions should you perform? Each correct answer presents part of the solution.

NOTE: Each correct selection is worth one point.

 

A.      Open the Advanced Editor.

B.      Select Column quality.

C.      Enable Column profiling based on entire dataset.

D.      Select Column distribution.

E.      Select Column profile.

 

Correct Answer: DE?

Data Profiling, Quality & Distribution in Power BI / Power Query features

To enable these features, you need to go to the View tab ֳ Data Preview Group ֳ Check the following:

✑ Column quality

✑ Column profile

✑ Column distribution

 

![Table  Description automatically generated with medium confidence](file:///C:/Users/V-WENB~1.FAR/AppData/Local/Temp/msohtmlclip1/01/clip_image003.png)

 

✑ Column profile

Turn on the Column Profiling feature.

 

![Graphical user interface, text, application  Description automatically generated](file:///C:/Users/V-WENB~1.FAR/AppData/Local/Temp/msohtmlclip1/01/clip_image005.png)

 

✑ Column distribution

Can use it to visually realize that your query is missing some data because of distinct and uniqueness counts.

![img](file:///C:/Users/V-WENB~1.FAR/AppData/Local/Temp/msohtmlclip1/01/clip_image007.png)

Reference:

https://www.thepoweruser.com/2019/08/13/data-profiling-quality-distribution-in-power-bi-power-query/

https://www.altentertraining.com/microsoft/power-bi/column-profiling-is-good/

 

C and E

Why C: In Power Query Editor, the default is Column Profiling on top 1000 rows (check the the left-hand-side conner to find this infor). We need t change it to Column Profiling on entire dataset to see 1500 rows.

Why E but not B?

B. Column Quality shows the PERCENTAGE of empty values

E. While E Column Profile will show the NUMBER of empty value in Column Statistic part and show the distribution of each Region/State in Value Distribution part.--> better infor than B.

C & E. I tried on one of the data set , I had the option to enable the Column profiling on the entire dataset and selected column profile option. Then was able to see the distribution of the data (under value distribution) for each state.

 

 

## #6

HOTSPOT -

You have an API that returns more than 100 columns. The following is a sample of column names.

✑ client_notified_timestamp

✑ client_notified_source

✑ client_notified_sourceid

✑ client_notified_value

✑ client_responded_timestamp

✑ client_responded_source

✑ client_responded_sourceid

✑ client_responded_value

You plan to include only a subset of the returned columns.

You need to remove any columns that have a suffix of sourceid.

How should you complete the Power Query M code? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point.

Hot Area:

 

![Graphical user interface, text, application  Description automatically generated](file:///C:/Users/V-WENB~1.FAR/AppData/Local/Temp/msohtmlclip1/01/clip_image009.png)

 

Box 1: Table.RemoveColumns -

When you do ג€Remove Columnsג€ Power Query uses the Table.RemoveColumns function

 

Box 2: List.Select - Get a list of columns.

 

Box 3: Text.Contains -

Example code to remove columns with a slash (/):

let

Source = Excel.Workbook(File.Contents("C: Source"), null, true), #"1_Sheet" = Source{[Item="1",Kind="Sheet"]}[Data],

\#"Promoted Headers" = Table.PromoteHeaders(#"1_Sheet", [PromoteAllScalars=true]),

/ get columns which contains any slash among values ColumnsToRemove =

List.Select(

/ get a list of all columns Table.ColumnNames(#"Promoted Headers"), (columnName) =>

let

/ get all values of a columns

ColumnValues = Table.Column(#"Promoted Headers", columnName),

/ go through values and stop when you find the first occurence of a text containing a slash

/ if there is a value with a slash, return true else false

ContainsSlash = List.AnyTrue(List.Transform(ColumnValues, each Text.Contains(_, "/"))) in

 

ContainsSlash -

),

/ remove columns

Result = Table.RemoveColumns(#"Promoted Headers", ColumnsToRemove) in

 

Result - Reference:

https://community.powerbi.com/t5/Power-Query/Remove-columns-containing-a-certain-value/td-p/759657

 

 

## #7

DRAG DROP -

You are building a dataset from a JSON file that contains an array of documents.

You need to import attributes as columns from all the documents in the JSON file. The solution must ensure that date attributes can be used as date hierarchies in

Microsoft Power BI reports.

Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.

Select and Place:

 

 

![Graphical user interface, application  Description automatically generated with medium confidence](file:///C:/Users/V-WENB~1.FAR/AppData/Local/Temp/msohtmlclip1/01/clip_image011.png)

**???**

 

Step 1: Expand the records.

First Open Power BI desktop and navigate to Power Query, import the JSON file, then load the data, click on the record to expand it and to see the record and list.

Step 2: Add columns that use data type conversions. Step 3: Convert the list to a table       

 

The correct answer is

1)covert list to Table

2)expand rows/records

3)Set Date type

The best way to see this is when you import a JSON file, by default, power bi will outline the steps in the applied steps pane. You will not see anything like expand the column

try it with this.

{

"employee" :[

{

"name" : "emp1", "age" : 20,

"date_naissance" : "11-09-1980"

},

{

"name":"emp2", "age":18,

"date_naissance" : "17-06-1980"

}

]

}

## #8

You import two Microsoft Excel tables named Customer and Address into Power Query. Customer contains the following columns:

✑ Customer ID

✑ Customer Name

✑ Phone

✑ Email Address

✑ Address ID

Address contains the following columns:

✑ Address ID

✑ Address Line 1

✑ Address Line 2

✑ City

✑ State/Region

✑ Country

✑ Postal Code

The Customer ID and Address ID columns represent unique rows.

You need to create a query that has one row per customer. Each row must contain City, State/Region, and Country for each customer. What should you do?

 

A.      Merge the Customer and Address tables.

B.      Transpose the Customer and Address tables.

C.      Group the Customer and Address tables by the Address ID column.

D.      Append the Customer and Address tables.

 

Correct Answer: *A*

There are two primary ways of combining queries: merging and appending.

✑ When you have one or more columns that youג€™d like to add to another query, you merge the queries.

✑ When you have additional rows of data that youג€™d like to add to an existing query, you append the query. Reference:

https://docs.microsoft.com/en-us/power-bi/connect-data/desktop-shape-and-combine-data

 

## #9

You have the following three versions of an Azure SQL database:

✑ Test

✑ Production

✑ Development

You have a dataset that uses the development database as a data source.

You need to configure the dataset so that you can easily change the data source between the development, test, and production database servers from powerbi.com.

Which should you do?

 

A.      Create a JSON file that contains the database server names. Import the JSON file to the dataset.

B.      Create a parameter and update the queries to use the parameter.

C.      Create a query for each database server and hide the development tables.

D.      Set the data source privacy level to Organizational and use the ReplaceValue Power Query M function.

Correct Answer: B

As you can't edit datasets data sources in Power BI service, we recommend using parameters to store connection details such as instance names and database names, instead of using a static connection string. This allows you to manage the connections through the Power BI service web portal, or using APIs, at a later stage.

Reference:

https://docs.microsoft.com/en-us/power-bi/create-reports/deployment-pipelines-best-practices

 

 

 

## #10

You have a CSV file that contains user complaints. The file contains a column named Logged. Logged contains the date and time each complaint occurred. The data in Logged is in the following format: 2018-12-31 at 08:59.

You need to be able to analyze the complaints by the logged date and use a built-in date hierarchy. What should you do?

 

A.      Change the data type of the Logged column to Date.

B.      Apply a transformation to extract the last 11 characters of the Logged column and set the data type of the new column to Date.

C.      Create a column by example that starts with 2018-12-31 and set the data type of the new column to Date.

D.      Add a conditional column that outputs 2018 if the Logged column starts with 2018 and set the data type of the new column to Whole Number.

 

Correct Answer: C

To use a built-in-date hierarchy, you need to set the data type of the new column to Date. Reference:

https://docs.microsoft.com/en-us/power-bi/create-reports/desktop-add-column-from-example 

https://www.exceljetconsult.com.ng/home/blog/power-query-split-date-and-time-into-separate-columns/

C is the correct answer. Why?

A.      there is fucking "at" in the format 2018-12-31 at 08:59 -->could not convert to date/time

B.      Last 11 characters?? --> should be first 11 characters instead

D. still the "at" issue"

## #11

 

You have an Azure SQL database that contains sales transactions. The database is updated frequently.

You need to generate reports from the data to detect fraudulent transactions. The data must be visible within five minutes of an update. How should you configure the data connection?

 

A.      Add a SQL statement.

B.      Set Data Connectivity mode to DirectQuery.

C.      Set the Command timeout in minutes setting.

D.      Set Data Connectivity mode to Import.

 

Correct Answer: B

With Power BI Desktop, when you connect to your data source, it's always possible to import a copy of the data into the Power BI Desktop. For some data sources, an alternative approach is available: connect directly to the data source using DirectQuery.

DirectQuery: No data is imported or copied into Power BI Desktop. For relational sources, the selected tables and columns appear in the Fields list. For multi- dimensional sources like SAP Business Warehouse, the dimensions and measures of the selected cube appear in the Fields list. As you create or interact with a visualization, Power BI Desktop queries the underlying data source, so youג€™re always viewing current data. Incorrect Answers:

D: Import: The selected tables and columns are imported into Power BI Desktop. As you create or interact with a visualization, Power BI Desktop uses the imported data. To see underlying data changes since the initial import or the most recent refresh, you must refresh the data, which imports the full dataset again.

Reference:

https://docs.microsoft.com/en-us/power-bi/connect-data/desktop-use-directquery

 

## #12

You have a data model that contains many complex DAX expressions. The expressions contain frequent references to the RELATED and RELATEDTABLE functions.

You need to recommend a solution to minimize the use of the RELATED and RELATEDTABLE functions.

What should you recommend?

A. Split the model into multiple models.

B.  Hide unused columns in the model.

C. Merge tables by using Power Query.

D. Transpose the required columns.

 

 

**Correct Answer:** *C*

Combining data means connecting to two or more data sources, shaping them as needed, then consolidating them into a useful query. When you have one or more columns that youג€™d like to add to another query, you merge the queries.

Note: The RELATEDTABLE function is a shortcut for CALCULATETABLE function with no logical expression. CALCULATETABLE evaluates a table expression in a modified filter context and returns A table of values.

Reference:

https://docs.microsoft.com/en-us/power-bi/connect-data/desktop-shape-and-combine-data

 

## #13

You have a large dataset that contains more than 1 million rows. The table has a datetime column named Date. You need to reduce the size of the data model without losing access to any data.

What should you do?

 

A.      Round the hour of the Date column to startOfHour.

B.      Change the data type of the Date column to Text.

C.      Trim the Date column.

D.      Split the Date column into two columns, one that contains only the time and another that contains only the date.

 

 

Correct Answer: D

We have to separate date & time tables. Also, we donג€™t need to put the time into the date table, because the time is repeated every day. Split your DateTime column into a separate date & time columns in fact table, so that you can join the date to the date table & the time to the time table. The time need to be converted to the nearest round minute or second so that every time in your data corresponds to a row in your time table.

Reference:

https://intellipaat.com/community/6461/how-to-include-time-in-date-hierarchy-in-power-bi

https://apexinsights.net/blog/top-5-tips-to-optimise- data-model

 

D - is the correct answer.

If you have a date time type column, probability you have much more unique values in each row, and consequently lower the compression capabilities of the engine.

 

 

## #14

DRAG DROP -

You are modeling data in a table named SalesDetail by using Microsoft Power BI.

You need to provide end users with access to the summary statistics about the SalesDetail data. The users require insights on the completeness of the data and the value distributions.

Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.

Select and Place:

![Graphical user interface, text, application, email  Description automatically generated](file:///C:/Users/V-WENB~1.FAR/AppData/Local/Temp/msohtmlclip1/01/clip_image013.png)

Step 1: Create a blank query as a data source

Start with a New Source in Power Query Editor, and then Blank Query

![Graphical user interface, application  Description automatically generated](file:///C:/Users/V-WENB~1.FAR/AppData/Local/Temp/msohtmlclip1/01/clip_image014.png)

Create a parameter that use a query for suggested values.

Step 2: Specify the following query, then close and apply. -Table.Profile(#ֲ¨SalesDetail")

In the new blank query, in the formula bar (if you donג€™t see the formula bar, check the formula bar option in the View tab of the Power Query Editor), type below expression:

=Table.Profile()

Note that this code is not complete yet, we need to provide a table as the input of this function.

Note: The Table.Profile() function takes a value of type table and returns a table that displays, for each column in the original table, the minimum, maximum, average, standard deviation, count of values, count of null values and count of distinct values.

Step 3: Create a visual for the query table.

The profiling data that you get from Table.Profile function is like below;

![Graphical user interface, application, table, Excel  Description automatically generated](file:///C:/Users/V-WENB~1.FAR/AppData/Local/Temp/msohtmlclip1/01/clip_image016.png)

After loading the data into Power BI, youג€™ll have the table with all columns, and it can be used in any visuals. Reference:

https://radacad.com/create-a-profiling-report-in-power-bi-give-the-end-user-information-about-the-data

## #15

You create the following step by using Power Query Editor.

\- Table.ReplaceValue(SalesLT_Address,"1318","1319",Replacer.ReplaceText,{"AddressLine1"}) A row has a value of 21318 Lasalle Street in the AddressLine1 column.

What will the value be when the step is applied?

 

A. 1318

B. 1319

C. 21318 Lasalle Street

D. 21319 Lasalle Street

 

Correct Answer: D

Example:

Replace the text "ur" with the text "or" in the table.

![Text  Description automatically generated](file:///C:/Users/V-WENB~1.FAR/AppData/Local/Temp/msohtmlclip1/01/clip_image017.png)

Reference:

https://docs.microsoft.com/en-us/powerquery-m/table-replacevalue

answer is correct Table.ReplaceValue( Table.FromRecords({

[a = 1, b = "hello"],

[a = 3, b = "goodbye"]

}),

"goodbye", "world",

Replacer.ReplaceText,

{"b"}

)

 

## #16

You have a Microsoft Power BI report. The size of PBIX file is 550 MB. The report is accessed by using an App workspace in shared capacity of powerbi.com.

The report uses an imported dataset that contains one fact table. The fact table contains 12 million rows. The dataset is scheduled to refresh twice a day at 08:00 and 17:00.

The report is a single page that contains 15 AppSource visuals and 10 default visuals.

Users say that the report is slow to load the visuals when they access and interact with the report. You need to recommend a solution to improve the performance of the report.

What should you recommend?

 

A.      Increase the number of times that the dataset is refreshed.

B.      Split the visuals onto multiple pages.

C.      Change the imported dataset to DirectQuery.

D.      Implement row-level security (RLS).

 

**B/C?**

**DirectQuery: No data is imported or copied into Power BI Desktop.**

**Import: The selected tables and columns are imported into Power BI Desktop. As you create or interact with a visualization, Power BI Desktop uses the imported data.**

 

**Benefits of using DirectQuery -**

**There are a few benefits to using DirectQuery:**

**✑ DirectQuery lets you build visualizations over very large datasets, where it would otherwise be unfeasible to first import all the data with pre- aggregation.**

**✑ Underlying data changes can require a refresh of data. For some reports, the need to display current data can require large data transfers,**

**making reimporting data unfeasible. By contrast, DirectQuery reports always use current data. The 1-GB dataset limitation doesn't apply to DirectQuery.**

**Note:**

**There are several versions of this question in the exam. The question can have other incorrect answer options, include the following:**

**✑ Implement row-level security (RLS)**

**✑ Increase the number of times that the dataset is refreshed. Reference:**

**https://docs.microsoft.com/en-us/power-bi/connect-data/desktop-use-directquery**

The answer is definitely B. To start, there are 25 visuals of which 15 are custom so those visuals are not optimized like the default ones. Even if all 2 visuals were fast wouldn't change the fact that Direct Query cannot help when you have that many visuals on screen. Each visual will have at least one query sent back to the source. Break it up onto different pages will equal less visuals to render. There's no need to change from import unless the data needs to be closer to real-time or refreshes are failing.

 

Accoring to this https://docs.microsoft.com/en-us/power-bi/connect-data/desktop-use-directquery If over 1 million rows are returned from DirectQuery, Power BI returns an error, so I think the answer is B

 

## #17

You create a dashboard by using the Microsoft Power BI Service. The dashboard contains a card visual that shows total sales from the current year.

You grant users access to the dashboard by using the Viewer role on the workspace. A user wants to receive daily notifications of the number shown on the card visual. You need to automate the notifications.

What should you do?

 

A.      Create a data alert.

B.      Share the dashboard to the user.

C.      Create a subscription.

D.      Tag the user in a comment.

 

Correct Answer: C

You can subscribe yourself and your colleagues to the report pages, dashboards, and paginated reports that matter most to you. Power BI e- mail subscriptions allow you to:

✑ Decide how often you want to receive the emails: daily, weekly, hourly, monthly, or once a day after the initial data refresh.

✑ Choose the time you want to receive the email, if you choose daily, weekly, hourly, or monthly.

Note: Email subscriptions don't support most custom visuals. The one exception is those custom visuals that have been certified. Email subscriptions don't support R-powered custom visuals at this time.

Incorrect Answers:

A: Set data alerts to notify you when data in your dashboards changes beyond limits you set. Reference:

https://docs.microsoft.com/en-us/power-bi/collaborate-share/service-report-subscribe https://docs.microsoft.com/en-us/power-bi/create- reports/service-set-data-alerts

 

The correct answer is C. Create a subscription. The requirement is to receive daily notifications of the number shown on the card visual. By using subscription, it can send an email with an image of the report which contains the number shown on the card visual. https://docs.microsoft.com/en-us/power-bi/collaborate-share/service-report-subscribe

 

For data alerts, although it can also send an notification daily, it will only send the notification unless the threshold hit. Well, you may use a very tricky way to set a threshold of > 0 to force it to send alert. However, the notification sent by data alert contains only the change in text but the requirement is notifications with number shown on the card visual, which is the dashboard itself. So, C. Create a subscription. is the correct answer https://docs.microsoft.com/en-us/power-bi/create-reports/service-set-data-alerts

 

## #18

Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.

After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You are modeling data by using Microsoft Power BI. Part of the data model is a large Microsoft SQL Server table named Order that has more than 100 million records.

During the development process, you need to import a sample of the data from the Order table. Solution: From Power Query Editor, you import the table and then add a filter step to the query. Does this meet the goal?

 

A.      Yes

B.      No

 

Correct Answer: B

The filter is applied after the data is imported. Instead add a WHERE clause to the SQL statement. Reference:

https://docs.microsoft.com/en-us/power-bi/connect-data/service-gateway-sql-tutorial

 

I think it's A because of query folding: the source is SQL Server, so with all steps applied a query will be generated and passed to the server (including the filter), so the result would be the same than including the WHERE clause in the query

upvoted 52 times

 

  eurekamike 1 month, 3 weeks ago

Correct. The main source of confusion is that is says import THEN filter.

The import part refers to the first step in the query which is specifying the source. It's talking about retrieving the data and importing it into the query mashup engine, not power BI. Import means Select. Very poor choice of words on their part.

 

Then we add the filter function as another step.

Since filter is supported by query folding, the query engine transforms the query steps into a single query. Then it loads the data into PBI.

 

Key thing to look for is that it asks you to put the filter in the same query that is importing (AKA Selecting) the data.

 

Ok.

 

 

 

3 days, 13 hours ago

 

Here is a FULL REASON why B is correct after You read much of this and still are wondering.

 

A is partially correct but. At the very first time when You decide not to use an SQL Statement immedietaly, a full import is done once. You don't need to wait for it to end, instead You can filter and the full import is cancelled so the filtered "folded" import is started.

 

So A is "eventually" correct too yet it does make a full import attempt once at the time of development. If You decide to put the SQL statement in the Connect window, the full import isn't even attempted.

 

## #19

Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.

After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You are modeling data by using Microsoft Power BI. Part of the data model is a large Microsoft SQL Server table named Order that has more than 100 million records.

During the development process, you need to import a sample of the data from the Order table. Solution: You add a WHERE clause to the SQL statement.

Does this meet the goal?

 

A.      Yes

B.      No

Correct Answer: A

The WHERE clause has its effects before the data is imported. Reference:

https://docs.microsoft.com/en-us/power-bi/connect-data/service-gateway-sql-tutorial

 

## #20

 

Correct Answer: *B*

Instead modify the source step of the queries to use DataSourceExcel as the file path.

Note: Parameterising a Data Source could be used in many different use cases. From connecting to different data sources defined in Query Parameters to load different combinations of columns.

Reference:

https:/[/www.biinsight.com/power-bi-desktop-query-parameters-part-1/](http://www.biinsight.com/power-bi-desktop-query-parameters-part-1/)

https://wkrzywiec.medium.com/passing-source-folder-path-as-parameter-to-query-code-in-power-query-19ec60797d94

 

## #21

Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.

After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You create a parameter named DataSourceExcel that holds the file name and location of a Microsoft Excel data source.

You need to update the query to reference the parameter instead of multiple hard-coded copies of the location within each query definition. Solution: You modify the source step of the queries to use DataSourceExcel as the file path.

Does this meet the goal?

 

A.      Yes

B.      No

 

 

Correct Answer: A

Parameterising a Data Source could be used in many different use cases. From connecting to different data sources defined in Query Parameters to load different combinations of columns.

Reference:

https://www.biinsight.com/power-bi-desktop-query-parameters-part-1/

[https://www.youtube.com/watch?v=ppM-mLckQfs&ab_channel=Curbal](http://www.youtube.com/watch?v=ppM-mLckQfs&ab_channel=Curbal)

 

## #22

Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.

After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You create a parameter named DataSourceExcel that holds the file name and location of a Microsoft Excel data source.

You need to update the query to reference the parameter instead of multiple hard-coded copies of the location within each query definition. Solution: You create a new query that references DataSourceExcel.

Does this meet the goal?

 

A.      Yes

B.      No

 

 

Correct Answer: B

Instead modify the source step of the queries to use DataSourceExcel as the file path.

Note: Parameterising a Data Source could be used in many different use cases. From connecting to different data sources defined in Query Parameters to load different combinations of columns.

Reference:

https://www.biinsight.com/power-bi-desktop-query-parameters-part-1/

 

## #23

 

Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.

After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You are modeling data by using Microsoft Power BI. Part of the data model is a large Microsoft SQL Server table named Order that has more than 100 million records.

During the development process, you need to import a sample of the data from the Order table. Solution: You add a report-level filter that filters based on the order date.

Does this meet the goal?

 

A. Yes

B.  No

 

**Correct Answer:** *B*

The filter is applied after the data is imported. Instead add a WHERE clause to the SQL statement. Reference:

https://docs.microsoft.com/en-us/power-bi/connect-data/service-gateway-sql-tutorial

## #24

Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.

After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen. You are modeling data by using Microsoft Power BI. Part of the data model is a large Microsoft SQL Server table named Order that has more than 100 million records.

During the development process, you need to import a sample of the data from the Order table. Solution: You write a DAX expression that uses the FILTER function.

Does this meet the goal?

 

A.      Yes

B.      No

Correct Answer: B

The filter is applied after the data is imported. Instead add a WHERE clause to the SQL statement. Reference:

https://docs.microsoft.com/en-us/power-bi/connect-data/service-gateway-sql-tutorial

 

You have a Power BI dashboard that monitors the quality of manufacturing processes. The dashboard contains the following elements: A line chart that shows the number of defective products manufactured by day.

 

✑ A KPI visual that shows the current daily percentage of defective products manufactured.

You need to be notified when the daily percentage of defective products manufactured exceeds 3%. What should you create?

 

A.      a Q&A visual

B.      a subscription

C.      a smart narrative visual

D.      an alert

 

 

Correct Answer: D

Reference:

https://docs.microsoft.com/en-us/power-bi/consumer/end-user-alerts

 

 

 

 

## #28

You are reviewing a query that produces 10,000 rows in the Power Query Editor. You need to identify whether a column contains only unique values.

Which two Data Preview options can you use? Each correct answer presents a complete solution. NOTE: Each correct selection is worth one point.

 

A.      Column profile

B.      Column distribution

C.      Show whitespace

D.      Column quality

E.      Monospaced

Correct Answer: AB

B: Column distribution: This feature provides a set of visuals underneath the names of the columns that showcase the frequency and distribution of the values in each of the columns. The data in these visualizations is sorted in descending order from the value with the highest frequency.

By hovering over the distribution data in any of the columns, you get information about the overall data in the column (with distinct count and unique values).

A: Column profile: This feature provides a more in-depth look at the data in a column [compared to column distribution]. Apart from the column distribution chart, it contains a column statistics chart.

Incorrect Answers:

 

D: Column quality -

The column quality feature labels values in rows in three categories:

✑ Valid, shown in green Error, shown in red -

✑ Empty, shown in dark grey Reference:

https://docs.microsoft.com/en-us/power-query/data-profiling-tools

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 